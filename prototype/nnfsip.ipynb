{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ActivationType(Enum):\n",
    "    LAYER_SIGMOID = 0\n",
    "    LAYER_RELU = 1\n",
    "    LAYER_SOFTMAX = 2\n",
    "    LAYER_LINEAR = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def deriv_ReLU(x):\n",
    "    return x > 0\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, layer_arch) -> None:\n",
    "        self.biases = []\n",
    "        self.weights = []\n",
    "        self.activation = []\n",
    "        previous_layer = layer_arch[0]\n",
    "        self.activation.append(ActivationType.LAYER_LINEAR)\n",
    "        for layer in layer_arch[1:]:\n",
    "            if isinstance(layer, ActivationType):\n",
    "                self.activation.append(layer)\n",
    "            else:\n",
    "                self.weights.append(np.random.rand(previous_layer, layer)-0.5)\n",
    "                self.biases.append(np.zeros(layer))\n",
    "                previous_layer = layer\n",
    "\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        output = x\n",
    "        for w, b, a in zip(self.weights, self.biases, self.activation):\n",
    "            output = np.dot(output, w)\n",
    "            output = np.add(output, b)\n",
    "            if a == ActivationType.LAYER_SIGMOID:\n",
    "                output = sigmoid(output)\n",
    "            elif a == ActivationType.LAYER_RELU:\n",
    "                output = ReLU(output)\n",
    "            elif a == ActivationType.LAYER_SOFTMAX:\n",
    "                output = softmax(output)\n",
    "            elif a == ActivationType.LAYER_LINEAR:\n",
    "                pass\n",
    "\n",
    "        return output\n",
    "\n",
    "    def MSE(self, Y_pred, Y_real):\n",
    "        X = np.square(Y_pred - Y_real)\n",
    "        return X / Y_pred.size\n",
    "\n",
    "    def backpropagate(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('mnist/mnist_train.csv')\n",
    "test_data = pd.read_csv('mnist/mnist_test.csv')\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "Y_train = train_data[:, 0]\n",
    "X_train = train_data[:, 1:train_data.shape[0]]\n",
    "X_train = X_train / 255\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "Y_test = test_data[:, 0]\n",
    "X_test = test_data[:, 1:test_data.shape[0]]\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(data, label):\n",
    "    output = data.reshape((28,28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(output, interpolation='nearest')\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(X_test[2], Y_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "nno = [\n",
    "    784,\n",
    "    10,\n",
    "    ActivationType.LAYER_SIGMOID,\n",
    "    10,\n",
    "    ActivationType.LAYER_SOFTMAX,\n",
    "]\n",
    "\n",
    "nn = NN(nno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.459764044429958"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = nn.feedforward(X_train[0])\n",
    "Y = np.zeros(10)\n",
    "Y[Y_train[0]] = Y_train[0]\n",
    "# nn.MSE(res, Y_train[0])\n",
    "res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\tdef __init__(self, layers, alpha=0.1):\n",
    "\t\t# initialize the list of weights matrices, then store the\n",
    "\t\t# network architecture and learning rate\n",
    "\t\tself.W = []\n",
    "\t\tself.layers = layers\n",
    "\t\tself.alpha = alpha\n",
    "        # start looping from the index of the first layer but\n",
    "\t\t# stop before we reach the last two layers\n",
    "\t\tfor i in np.arange(0, len(layers) - 2):\n",
    "\t\t\t# randomly initialize a weight matrix connecting the\n",
    "\t\t\t# number of nodes in each respective layer together,\n",
    "\t\t\t# adding an extra node for the bias\n",
    "\t\t\tw = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "\t\t\tself.W.append(w / np.sqrt(layers[i]))\n",
    "\t\t# the last two layers are a special case where the input\n",
    "\t\t# connections need a bias term but the output does not\n",
    "\t\tw = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "\t\tself.W.append(w / np.sqrt(layers[-2]))\n",
    "\tdef __repr__(self):\n",
    "\t\t# construct and return a string that represents the network\n",
    "\t\t# architecture\n",
    "\t\treturn \"NeuralNetwork: {}\".format(\n",
    "\t\t\t\"-\".join(str(l) for l in self.layers))\n",
    "\tdef sigmoid(self, x):\n",
    "\t\t# compute and return the sigmoid activation value for a\n",
    "\t\t# given input value\n",
    "\t\treturn 1.0 / (1 + np.exp(-x))\n",
    "\tdef sigmoid_deriv(self, x):\n",
    "\t\t# compute the derivative of the sigmoid function ASSUMING\n",
    "\t\t# that x has already been passed through the 'sigmoid'\n",
    "\t\t# function\n",
    "\t\treturn x * (1 - x)\n",
    "\tdef fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "\t\t# insert a column of 1's as the last entry in the feature\n",
    "\t\t# matrix -- this little trick allows us to treat the bias\n",
    "\t\t# as a trainable parameter within the weight matrix\n",
    "\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
    "\t\t# loop over the desired number of epochs\n",
    "\t\tfor epoch in np.arange(0, epochs):\n",
    "\t\t\t# loop over each individual data point and train\n",
    "\t\t\t# our network on it\n",
    "\t\t\tfor (x, target) in zip(X, y):\n",
    "\t\t\t\tself.fit_partial(x, target)\n",
    "\t\t\t# check to see if we should display a training update\n",
    "\t\t\tif epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "\t\t\t\tloss = self.calculate_loss(X, y)\n",
    "\t\t\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "\t\t\t\t\tepoch + 1, loss))\n",
    "\tdef fit_partial(self, x, y):\n",
    "\t\t# construct our list of output activations for each layer\n",
    "\t\t# as our data point flows through the network; the first\n",
    "\t\t# activation is a special case -- it's just the input\n",
    "\t\t# feature vector itself\n",
    "\t\tA = [np.atleast_2d(x)]\n",
    "\t\t# FEEDFORWARD:\n",
    "\t\t# loop over the layers in the network\n",
    "\t\tfor layer in np.arange(0, len(self.W)):\n",
    "\t\t\t# feedforward the activation at the current layer by\n",
    "\t\t\t# taking the dot product between the activation and\n",
    "\t\t\t# the weight matrix -- this is called the \"net input\"\n",
    "\t\t\t# to the current layer\n",
    "\t\t\tnet = A[layer].dot(self.W[layer])\n",
    "\t\t\t# computing the \"net output\" is simply applying our\n",
    "\t\t\t# nonlinear activation function to the net input\n",
    "\t\t\tout = self.sigmoid(net)\n",
    "\t\t\t# once we have the net output, add it to our list of\n",
    "\t\t\t# activations\n",
    "\t\t\tA.append(out)\n",
    "\t\t# BACKPROPAGATION\n",
    "\t\t# the first phase of backpropagation is to compute the\n",
    "\t\t# difference between our *prediction* (the final output\n",
    "\t\t# activation in the activations list) and the true target\n",
    "\t\t# value\n",
    "\t\terror = A[-1] - y\n",
    "\t\t# from here, we need to apply the chain rule and build our\n",
    "\t\t# list of deltas 'D'; the first entry in the deltas is\n",
    "\t\t# simply the error of the output layer times the derivative\n",
    "\t\t# of our activation function for the output value\n",
    "\t\tD = [error * self.sigmoid_deriv(A[-1])]\n",
    "\t\t# once you understand the chain rule it becomes super easy\n",
    "\t\t# to implement with a 'for' loop -- simply loop over the\n",
    "\t\t# layers in reverse order (ignoring the last two since we\n",
    "\t\t# already have taken them into account)\n",
    "\t\tfor layer in np.arange(len(A) - 2, 0, -1):\n",
    "\t\t\t# the delta for the current layer is equal to the delta\n",
    "\t\t\t# of the *previous layer* dotted with the weight matrix\n",
    "\t\t\t# of the current layer, followed by multiplying the delta\n",
    "\t\t\t# by the derivative of the nonlinear activation function\n",
    "\t\t\t# for the activations of the current layer\n",
    "\t\t\tdelta = D[-1].dot(self.W[layer].T)\n",
    "\t\t\tdelta = delta * self.sigmoid_deriv(A[layer])\n",
    "\t\t\tD.append(delta)\n",
    "  \t\t# since we looped over our layers in reverse order we need to\n",
    "\t\t# reverse the deltas\n",
    "\t\tD = D[::-1]\n",
    "\t\t# WEIGHT UPDATE PHASE\n",
    "\t\t# loop over the layers\n",
    "\t\tfor layer in np.arange(0, len(self.W)):\n",
    "\t\t\t# update our weights by taking the dot product of the layer\n",
    "\t\t\t# activations with their respective deltas, then multiplying\n",
    "\t\t\t# this value by some small learning rate and adding to our\n",
    "\t\t\t# weight matrix -- this is where the actual \"learning\" takes\n",
    "\t\t\t# place\n",
    "\t\t\tself.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "\tdef predict(self, X, addBias=True):\n",
    "\t\t# initialize the output prediction as the input features -- this\n",
    "\t\t# value will be (forward) propagated through the network to\n",
    "\t\t# obtain the final prediction\n",
    "\t\tp = np.atleast_2d(X)\n",
    "\t\t# check to see if the bias column should be added\n",
    "\t\tif addBias:\n",
    "\t\t\t# insert a column of 1's as the last entry in the feature\n",
    "\t\t\t# matrix (bias)\n",
    "\t\t\tp = np.c_[p, np.ones((p.shape[0]))]\n",
    "\t\t# loop over our layers in the network\n",
    "\t\tfor layer in np.arange(0, len(self.W)):\n",
    "\t\t\t# computing the output prediction is as simple as taking\n",
    "\t\t\t# the dot product between the current activation value 'p'\n",
    "\t\t\t# and the weight matrix associated with the current layer,\n",
    "\t\t\t# then passing this value through a nonlinear activation\n",
    "\t\t\t# function\n",
    "\t\t\tp = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "\t\t# return the predicted value\n",
    "\t\treturn p\n",
    "\tdef calculate_loss(self, X, targets):\n",
    "\t\t# make predictions for the input data points then compute\n",
    "\t\t# the loss\n",
    "\t\ttargets = np.atleast_2d(targets)\n",
    "\t\tpredictions = self.predict(X, addBias=False)\n",
    "\t\tloss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "\t\t# return the loss\n",
    "\t\treturn loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST (sample) dataset...\n",
      "[INFO] samples: 1797, dim: 64\n",
      "[INFO] training network...\n",
      "[INFO] NeuralNetwork: 64-32-16-10\n",
      "[INFO] epoch=1, loss=604.9447770\n",
      "[INFO] epoch=100, loss=8.3260316\n",
      "[INFO] epoch=200, loss=3.2229052\n",
      "[INFO] epoch=300, loss=2.5896911\n",
      "[INFO] epoch=400, loss=2.3756759\n",
      "[INFO] epoch=500, loss=1.7982862\n",
      "[INFO] epoch=600, loss=1.7247240\n",
      "[INFO] epoch=700, loss=1.6810033\n",
      "[INFO] epoch=800, loss=1.6513647\n",
      "[INFO] epoch=900, loss=1.6298778\n",
      "[INFO] epoch=1000, loss=1.6135822\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        41\n",
      "           1       0.98      0.95      0.96        56\n",
      "           2       0.98      1.00      0.99        44\n",
      "           3       0.97      0.97      0.97        33\n",
      "           4       1.00      0.98      0.99        47\n",
      "           5       1.00      0.95      0.97        60\n",
      "           6       1.00      1.00      1.00        40\n",
      "           7       1.00      0.98      0.99        46\n",
      "           8       0.88      0.97      0.93        39\n",
      "           9       0.96      1.00      0.98        44\n",
      "\n",
      "    accuracy                           0.98       450\n",
      "   macro avg       0.98      0.98      0.98       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "# load the MNIST dataset and apply min/max scaling to scale the\n",
    "# pixel intensity values to the range [0, 1] (each image is\n",
    "# represented by an 8 x 8 = 64-dim feature vector)\n",
    "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],\n",
    "\tdata.shape[1]))\n",
    "\n",
    "# construct the training and testing splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "\tdigits.target, test_size=0.25)\n",
    "# convert the labels from integers to vectors\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "nn = NeuralNetwork([trainX.shape[1], 32, 16, 10])\n",
    "print(\"[INFO] {}\".format(nn))\n",
    "nn.fit(trainX, trainY, epochs=1000)\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = nn.predict(testX)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.18823016e-05, 2.60575406e-03, 9.22449772e-04, 6.79064823e-05,\n",
       "        3.72389280e-06, 1.21253008e-04, 8.84831659e-04, 8.23741116e-06,\n",
       "        9.99680452e-01, 1.01540178e-03]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(testX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
